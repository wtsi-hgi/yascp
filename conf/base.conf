/*
========================================================================================
    nf-core/yascp Nextflow base config file
========================================================================================
    A 'blank slate' config file, appropriate for general use on most high performance
    compute environments. Assumes that all software is installed and available on
    the PATH. Runs in `local` mode - all jobs will be run on the logged in environment.
----------------------------------------------------------------------------------------
*/

params{
    input = 'existing_cellbender' 
    rsync_to_web_file="${launchDir}/yascp/bin/rsync_to_web.sh"
    profile='normal_run'
    citeseq=false
    //# These are default parameters that can be overwriten to run in a different mode.
    //# Here we have listed the default parameters when running without any extrainput.
    tmpdir = "${launchDir}/work"
    cohorts_to_drop_from_GT_Relatednes_check=''
    provide_within_pool_donor_specific_sites_for_pilup=true
    hard_filters_file = "no_file__file_sample_qc" //# This may point to the sample_qc.yml input which will apply hard filters to the merged cells.
    hard_filters_drop = false //#This indicates whether we want to drop the cells that fail hard filters of just flag them
    drop_cell_passes_qc_from_clustering = false //#This filter if set to true will drop the cells that dont pass the adaptive qc criteria before clustering.
    encrypt = false
    write_h5=false
    remove_work_dir = false
    cellbender_location="${launchDir}/results"
    skip_handover = false
    RUN='default'
    skip_qc=false
    skip_merge=false
    just_reports=false
    add_donor_metadata = false
    cellex_cluster_markers=false
    mem1= 12000
    copy_mode = "rellink"
    split_bam = false
    existing_cellsnp="${projectDir}/assets/existing_cellsnp"
    existing_vireo=''
    skip_preprocessing{
        value=false
        gt_match_file="" // #We prvide this if we want to exclude a particular samples matched to a ceirtain GT cohortc from the adaptive qc
        gt_match_based_adaptive_qc_exclusion_pattern = '' // #We run the adaptive QC on these patterns independently regardless on assigned celltype.        
        file__anndata_merged = ''
        file__cells_filtered = ''
    }
    genotype_phenotype_mapping_file =''
    extra_sample_metadata = ''
    use_phenotype_ids_for_gt_match = true //#if false this will keep the genotype ids, for this to be used have to set a genotype_phenotype_mapping_file to a path to csv where firs column contains genotype ids and second contains phenotype ids to replace these to.
    run_celltype_assignment = true
    cluster_validate_resolution_keras = false
    input_tables_column_delimiter = '\t'
    output_dir = outdir= "${launchDir}/results"
    do_deconvolution = true
    split_bam = false
    run_multiplet = true
    utilise_gpu = true
    input = 'existing_cellbender'
    split_ad_per_bach = false
    cellbender_resolution_to_use='0pt1'
    reference_assembly_fasta_dir = "https://yascp.cog.sanger.ac.uk/public/10x_reference_assembly"
    webtransfer = false
    project_name = 'Cardinal_pilots'
    run_with_genotype_input=false

    eQTL{
        eqtl_container = 'https://yascp.cog.sanger.ac.uk/public/singularity_images/eqtl_26_10_2022.img'
        aggregation_collumn='Azimuth:predicted.celltype.l2'
        n_min_cells = '5' // #The number of cells for individual to use. 
        n_min_individ = '30' // #Do not select less than 25 since this may result in a permutation issue with tensorqtl
        aggregation_method = 'dMean,dSum'
    }
    just_overlapping_positions_for_study_merge=false
	genotype_input {
        subset_genotypes = false // # if activated this will use the IDs provided in the input.tsv to to perform the GT match against, otherwise it will match against full cohort.
        run_with_genotype_input= false // #Whether we are using genotypes in our runs.
        vireo_with_gt=false // #if activated this will run vireo with genotypes.
        subset_vireo_genotypes = true // # This is a switch that determines whether we want to provide full genotype file in the vireo as an input or subset it down to the expected donors. NOTE: you may want to provide already merged shards for this, otherwise pipeline will merge this for you.
        posterior_assignment = false // #if this is set to true, we will perform the genotype donor matching after the deconvolution is performed.
        tsv_donor_panel_vcfs = "" // #tlist of vcf/bcf inputs 1) Can be a single file 2) Can be multiple cohorts (in cases where we dont want to merge the genotypes together) 3) Can be sharded inputs (for example per chromosome)
    }

    cellsnp {
        run = true
        remove_workdir = false
        copy_mode = "rellink"
        vcf_candidate_snps = "https://yascp.cog.sanger.ac.uk/public/cellsnp/genome1K.phase3.SNP_AF5e2.chr1toX.hg38.vcf.gz"
        description = """// this list of candidate SNPs for cellSNP comes from link at https://github.com/single-cell-genetics/cellSNP
        // i.e., https://sourceforge.net/projects/cellsnp/files/SNPlist/genome1K.phase3.SNP_AF5e2.chr1toX.hg38.vcf.gz/download"""
        min_maf = "0.1"
        min_count = "20"
        p = "20"
    }

    vireo {
        run = true
        remove_workdir = false
        copy_mode = "rellink"
        run_gtmatch_aposteriori = true
        subsample_times=10
        rate=60
   }

    plot_donor_ncells {
        run = false
        remove_workdir = false
        copy_mode = "rellink"
        plotnine_dpi = "100"
    }

    souporcell {
        run = true
        use_raw_barcodes = false
        remove_workdir = false
        copy_mode = "rellink"
        reference_fasta = "https://yascp.cog.sanger.ac.uk/public/10x_reference_assembly/genome.fa"
     }


    plot_souporcell_vs_vireo {
        run = false
        remove_workdir = false
        copy_mode = "rellink"
    }

    cellsnp_recapture ='1'
    split_h5ad_per_donor {
        run = true
        remove_workdir = false
        copy_mode = "rellink"
        input_h5_genome_version = "GRCh38"
        print_modules_version = "True"
        plot_n_cells_per_vireo_donor = "True"
        write_donor_level_filtered_cells_h5 = "True"
        plotnine_dpi = "100"
        anndata_compression_level = "6"
    }

}

process {
    cache = 'lenient'

    cpus   = {  1    * task.attempt }
    memory = {  6.GB * task.attempt }
    time   = { 4.h  * task.attempt }
    containerOptions = " --cleanenv --containall -B "+params.tmpdir+":/tmp --env NUMBA_CACHE_DIR='"+params.tmpdir+"' --env MPLCONFIGDIR='"+params.tmpdir+"'"
    
    //# errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries    = 5
    maxErrors     = '-1'
    errorStrategy = 'retry'
    // #Process-specific resource requirements
    // #NOTE - Please try and re-use the labels below as much as possible.
    // #       These labels are used and recognised by default in DSL2 files hosted on nf-core/modules.
    // #       If possible, it would be nice to keep the same label naming convention when
    // #       adding in your local modules too.
    // # The queues differ between institutions. So please chence them according to the times.

    withLabel:process_tiny {
      cpus = 1
      maxRetries    = 5
      memory = 2.GB
      time   = {  3.h   * task.attempt }
    }
    withLabel:process_low {
        maxRetries    = 3
        cpus   = { 2     * task.attempt }
        memory = { 12.GB * task.attempt }
        time   = {  4.h   * task.attempt }
    }
    withLabel:medium_cpus {
        cpus   = {  2     * task.attempt }
        memory = { 36.GB * task.attempt }
        time   = {  12.h   * task.attempt }
    }

    withLabel:process_medium {
        cpus   = {  1     * task.attempt }
        memory = { 36.GB * task.attempt }
        time   = {  12.h   * task.attempt }
        maxRetries    = 3
    }

    withLabel:process_medium_single_CPU {
        cpus   = { 1     * task.attempt }
    }
    withLabel:many_cores_small_mem {
        cpus   = {  20     * task.attempt }
        memory = { 20.GB * task.attempt }
        time   = { 12.h   * task.attempt }
    }


    withLabel:process_high {
        cpus   = {  4    * task.attempt }
        memory = { 100.GB * task.attempt }
        time   = 48.h
    }
    withLabel:process_long {
        time   = 48.h
        memory = { 36.GB * task.attempt }
        cpus   = { 1     * task.attempt }
    }
    withLabel:process_extralong {
        time   = 78.h
    }
    withLabel:process_high_memory {
        memory = { 150.GB * task.attempt}
        maxRetries    = 5
        time   = { 12.h   * task.attempt }
    }
    withLabel:process_medium_memory {
        memory = { 30.GB * task.attempt }
    }

    withName: SCRUBLET{
        maxRetries    = 3
        errorStrategy = { task.attempt > 2 ? 'ignore' : 'retry' }
    }
    withName: AZIMUTH{
        maxRetries    = 3
        errorStrategy = { task.attempt > 2 ? 'ignore' : 'retry' }
    }
    withName: CELLTYPIST{
        maxRetries    = 3
        errorStrategy = { task.attempt > 2 ? 'ignore' : 'retry' }
    }
    withName: cellbender__remove_background{
        maxRetries    = 3
        errorStrategy = { task.attempt > 2 ? 'ignore' : 'retry' }
    }

    withLabel:error_ignore {
        errorStrategy = 'ignore'
    }
    withLabel:error_retry {
        errorStrategy = 'retry'
        maxRetries    = 2
    }

    //!!!!! # Thisis sanger specific definition of gpu queue - please change this according to your institution
    withLabel: gpu {
        cpus = 1
        maxForks=4
        errorStrategy = 'retry'
        queue = { task.attempt > 1 ? 'gpu-huge' : 'gpu-normal' }
        clusterOptions = { "-M "+params.mem1*task.attempt+" -R 'select[ngpus>0 && mem>="+params.mem1*task.attempt+"] rusage[ngpus_physical=1.00,mem="+params.mem1*task.attempt+"] span[ptile=1]' -gpu 'mode=exclusive_process'" }
	    memory = '' // set to null '' as already specified in clusterOptions

        time   = { 12.h   * task.attempt }
        containerOptions = {
            workflow.containerEngine == "singularity" ? '--containall --cleanenv --nv -B /tmp':
            ( workflow.containerEngine == "docker" ? '--gpus all': null )
        }
    }
    
    withName:cluster_validate_resolution_keras{
        maxForks=4
    }

    withName: CELLTYPIST{
        maxForks=7
    }

    withName: CELLTYPE_FILE_MERGE{
        memory = { 60.GB * task.attempt }
    }

    withName: NORMALISE_AND_PCA{
        maxForks=7
        errorStrategy = 'retry'
        memory = { 50.GB * task.attempt}
        maxRetries    = 8
        cpus   = 4
    }
    
    withName: LISI{
        maxForks=7
        memory =300.GB
    }

    withName: OUTLIER_FILTER{
        errorStrategy = 'retry'
        memory = { 50.GB * task.attempt}
        maxRetries    = 8
    }
    

    withName: cluster{
        cpus   = {  3     * task.attempt }
    }

    withName: LISI{
        maxForks=7
        errorStrategy = 'retry'
        maxRetries    = 8
        memory = { 200.GB * task.attempt}
    }

    withName: VIREO_GT_FIX_HEADER{
        errorStrategy = 'retry'
        maxRetries    = 4
    }


    withName: JOIN_CHROMOSOMES{
        errorStrategy = 'retry'
        maxRetries    = 4
    }

    withName: cluster{
        errorStrategy = 'retry'
        maxRetries    = 4
    }

    withName: SPLIT_BAM_BY_CELL_BARCODES {
      cpus = 1
      memory = { 8.GB * task.attempt}
      time = 4.h
    }

    withName: CONCORDANCE_CALCLULATIONS{
        cpus = 5
        time   = {  12.h   * task.attempt }
        memory = { 50.GB * task.attempt }
    }

    withName: OTHER_DONOR_CONCORDANCE_CALCLULATIONS{
        cpus = 3
        time   = {  6.h   * task.attempt }
        memory = { 20.GB * task.attempt }
    }


    withName: DYNAMIC_DONOR_EXCLUSIVE_SNP_SELECTION{
        cpus = 5
        time   = { 12.h   * task.attempt }
        memory = { 50.GB * task.attempt }
    }

    withName: VIREO{
        //# maxForks=7
        cpus   = 5
        time   = { 12.h   * task.attempt }
        memory = { 70.GB * task.attempt }
    }

    withName: umap_gather{
        memory = { 100.GB * task.attempt }
        errorStrategy = 'retry'
        maxRetries    = 3
    }

    withName: plot_predicted_sex{
        memory = { 50.GB * task.attempt }
        maxRetries = 5
    }


}

singularity {
  enabled = true
  runOptions = '--bind /lustre --bind /software --bind /nfs --no-home'
}
