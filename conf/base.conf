/*
========================================================================================
    nf-core/yascp Nextflow base config file
========================================================================================
    A 'blank slate' config file, appropriate for general use on most high performance
    compute environments. Assumes that all software is installed and available on
    the PATH. Runs in `local` mode - all jobs will be run on the logged in environment.
----------------------------------------------------------------------------------------
*/

params{
    rsync_to_web_file="${launchDir}/yascp/bin/rsync_to_web.sh"
    //# These are default parameters that can be overwriten to run in a different mode.
    //# Here we have listed the default parameters when running without any extrainput.
    tmpdir = "${launchDir}/work"
    cohorts_to_drop_from_GT_Relatednes_check=''
    encrypt = false
    write_h5=false
    existing_vireo=false
    skip_handover = false
    RUN='default'
    skip_qc=false
    skip_merge=false
    just_reports=false
    add_donor_metadata = false
    cellex_cluster_markers=false
    mem1= 12000
    copy_mode = "rellink"
    split_bam = false
    
    existing_cellsnp=''
    
    //# If this is activated then we can skip all the cellbender and deconvolution and provide already merged h5ad.
    skip_preprocessing{
        value=false
        file__anndata_merged = ''
        file__cells_filtered = ''
    }
    genotype_phenotype_mapping_file =''
    extra_sample_metadata = ''
    use_phenotype_ids_for_gt_match = true //#if false this will keep the genotype ids, for this to be used have to set a genotype_phenotype_mapping_file to a path to csv where firs column contains genotype ids and second contains phenotype ids to replace these to.
    run_celltype_assignment = true
    cluster_validate_resolution_keras = false
    input_tables_column_delimiter = '\t'
    output_dir = outdir= "${launchDir}/results"
    do_deconvolution = true
    split_bam = false
    run_multiplet = true
    utilise_gpu = true
    input = 'cellbender'
    split_ad_per_bach = false
    cellbender_resolution_to_use='0pt1'
    reference_assembly_fasta_dir = "/lustre/scratch123/hgi/projects/ukbb_scrna/pipelines/Pilot_UKB/genotypes/10x_reference_assembly"
    //reference_genotype_vcf = "/lustre/scratch123/hgi/projects/ukbb_scrna/pipelines/Pilot_UKB/qc/Cherry_ELGH/gtmatch/1000G_full_GRCh38.srt.vcf.gz"
    webtransfer = false
    project_name = 'Cardinal_pilots'
    run_with_genotype_input=false
    
	genotype_input {
        posterior_assignment = false
        subset_genotypes = false 
        full_vcf_file = ''
    }

    cellsnp {
        run = true
        remove_workdir = false
        copy_mode = "rellink"
        vcf_candidate_snps = "/lustre/scratch123/hgi/projects/ukbb_scrna/pipelines/Pilot_UKB/qc/genome1K.phase3.SNP_AF5e2.chr1toX.hg38.vcf.gz"
        description = """// this list of candidate SNPs for cellSNP comes from link at https://github.com/single-cell-genetics/cellSNP
        // i.e., https://sourceforge.net/projects/cellsnp/files/SNPlist/genome1K.phase3.SNP_AF5e2.chr1toX.hg38.vcf.gz/download"""
        min_maf = "0.1"
        min_count = "60"
        p = "20"
    }

    vireo {
        run = true
        remove_workdir = false
        copy_mode = "rellink"
        run_gtmatch_aposteriori = true
   }

    plot_donor_ncells {
        run = false
        remove_workdir = false
        copy_mode = "rellink"
        plotnine_dpi = "100"
    }

    souporcell {
        run = true
        use_raw_barcodes = false
        remove_workdir = false
        copy_mode = "rellink"
        reference_fasta = "${params.igenomes_base}/Homo_sapiens/NCBI/GRCh38/Sequence/WholeGenomeFasta/genome.fa"
     }


    plot_souporcell_vs_vireo {
        run = false
        remove_workdir = false
        copy_mode = "rellink"
    }


    split_h5ad_per_donor {
        run = true
        remove_workdir = false
        copy_mode = "rellink"
        input_h5_genome_version = "GRCh38"
        print_modules_version = "True"
        plot_n_cells_per_vireo_donor = "True"
        write_donor_level_filtered_cells_h5 = "True"
        plotnine_dpi = "100"
        anndata_compression_level = "6"
    }


}

process {
    cache = 'lenient'
    // TODO nf-core: Check the defaults for all processes
    cpus   = { check_max( 1    * task.attempt, 'cpus'   ) }
    memory = { check_max( 6.GB * task.attempt, 'memory' ) }
    time   = { check_max( 4.h  * task.attempt, 'time'   ) }
    containerOptions = " --cleanenv --containall -B "+params.tmpdir+":/tmp --env NUMBA_CACHE_DIR='"+params.tmpdir+"' --env MPLCONFIGDIR='"+params.tmpdir+"'"

    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries    = 1
    maxErrors     = '-1'
    
    // Process-specific resource requirements
    // NOTE - Please try and re-use the labels below as much as possible.
    //        These labels are used and recognised by default in DSL2 files hosted on nf-core/modules.
    //        If possible, it would be nice to keep the same label naming convention when
    //        adding in your local modules too.
    // TODO nf-core: Customise requirements for specific processes.
    // See https://www.nextflow.io/docs/latest/config.html#config-process-selectors
    withLabel:process_tiny {
      cpus = 1
      memory = 1000.MB
      time = 1.h
    }
    withLabel:process_low {
        cpus   = { check_max( 2     * task.attempt, 'cpus'    ) }
        memory = { check_max( 12.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 4.h   * task.attempt, 'time'    ) }
        queue = { task.attempt > 3 ? 'long' : 'normal' }
    }
    withLabel:medium_cpus {
        cpus   = { check_max( 4     * task.attempt, 'cpus'    ) }
        memory = { check_max( 36.GB * task.attempt, 'memory'  ) }
    }

    withLabel:process_medium {
        cpus   = { check_max( 6     * task.attempt, 'cpus'    ) }
        memory = { check_max( 36.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 12.h   * task.attempt, 'time'    ) }
        queue = { task.attempt > 1 ? 'long' : 'normal' }
    }
    withLabel:process_medium_single_CPU {
        cpus   = { check_max( 1     * task.attempt, 'cpus'    ) }
        queue = { task.attempt > 1 ? 'long' : 'normal' }
    }
    withLabel:many_cores_small_mem {
        cpus   = { check_max( 20     * task.attempt, 'cpus'    ) }
        memory = { check_max( 20.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 12.h   * task.attempt, 'time'    ) }
        queue = { task.attempt > 1 ? 'long' : 'normal' }
    }

    withLabel:process_high {
        cpus   = { check_max( 12    * task.attempt, 'cpus'    ) }
        memory = { check_max( 100.GB * task.attempt, 'memory'  ) }
        time   = 48.h
        queue = 'long'
    }
    withLabel:process_long {
        time   = 48.h
        queue = 'long'
        memory = { check_max( 36.GB * task.attempt, 'memory'  ) }
        cpus   = { check_max( 4     * task.attempt, 'cpus'    ) }
    }
    withLabel:process_extralong {
        time   = 78.h
        queue = 'basement'
    }
    withLabel:process_high_memory {
        memory = { check_max( 200.GB * task.attempt, 'memory' ) }
    }
    withLabel:process_medium_memory {
        memory = { check_max( 100.GB * task.attempt, 'memory' ) }
    }
    withName: plot_distributions{
        containerOptions = "--containall --cleanenv --workdir /tmp -B /tmp"
    }

    withLabel:error_ignore {
        errorStrategy = 'ignore'
    }
    withLabel:error_retry {
        errorStrategy = 'retry'
        maxRetries    = 2
    }
    withLabel: gpu {
        cpus = 1
        maxForks=4

        errorStrategy = 'retry'
        queue = { task.attempt > 1 ? 'gpu-huge' : 'gpu-normal' }
        clusterOptions = { "-M "+params.mem1*task.attempt+" -R 'select[ngpus>0 && mem>="+params.mem1*task.attempt+"] rusage[ngpus_physical=1.00,mem="+params.mem1*task.attempt+"] span[ptile=1]' -gpu 'mode=exclusive_process'" }
	    memory = '' // set to null '' as already specified in clusterOptions

        time   = { check_max( 12.h   * task.attempt, 'time'    ) }
        containerOptions = {
            workflow.containerEngine == "singularity" ? '--containall --cleanenv --nv -B /tmp':
            ( workflow.containerEngine == "docker" ? '--gpus all': null )
        }
    }
    withName:cluster_validate_resolution_keras{
        maxForks=4
    }

    withName: CELLTYPIST{
        maxForks=7
    }

    withName: NORMALISE_AND_PCA{
        maxForks=7
        memory =200.GB
    }
    
    withName: LISI{
        maxForks=7
        memory =300.GB
    }

    withName: LISI{
        maxForks=7
        memory =300.GB
    }


    withName: SPLIT_BAM_BY_CELL_BARCODES {
      cpus = 4
      memory = { check_max( 8.GB * task.attempt, 'memory' ) }
      time = 4.h
      //stageOutMode = 'move'
    }
}
