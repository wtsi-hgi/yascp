-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/limix/main.nf =====

process CHUNK_GENOME{
    tag "$condition, $nr_phenotype_pcs"
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        each path(genome_annotation)
        tuple path(phenotype_pcs),val(condition),path(mapping_file),path(phenotype_file)
        val(chunkSize)

    output:
        tuple val(condition),path(phenotype_file), path(phenotype_pcs),path("Chunging_file*.tsv"),path(mapping_file) , emit: filtered_chunking_file optional true
    script:
        nr_phenotype_pcs = phenotype_pcs.getSimpleName()
        if ("${params.chromosomes_to_test}"!=''){
            chromosomes_as_string = params.chromosomes_to_test.join(',')
            cond2 = " --chr ${chromosomes_as_string}"
        }else{
            cond2 = " "
        }
        """
            generate_chunking_file.py --genome_annotation ${genome_annotation} --chunk_size ${chunkSize} --phenotype_file ${phenotype_file} --covar_file ${phenotype_pcs} --condition ${condition}  --genotype_phenotype_file ${mapping_file} ${cond2}
        """
    
}

process COLLECT_RESULTS{
    tag { condition }
    label 'process_tiny'
    input:
        tuple val(condition), path(metadata),path(full_output_list),path(feature_metadata)
    output:
        path("${condition}"), emit: condition_all_qtls
    script:
        full_output_list=full_output_list.join("\n")
        """
        echo -e "${full_output_list}">out.txt
        link_files.py --files_input out.txt --condition ${condition}
        """
}

process AGGREGATE_QTL_RESULTS{
    tag { condition }
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    
    

    publishDir  path: "${params.outdir}/Limix_eQTLS/${group1}__${group0}/${group2}",
                mode: "${params.copy_mode}",
                overwrite: "true"

    input:
        tuple val(condition), path(metadata),path(full_output_list),path(feature_metadata)
        
        
    output:
        path("results_${group1}"), emit: limix_qtl_path
        // path("${all_qtl_results}_all_mpr/qtl_results_all.txt"), emit: qtl_results_all_all_mpr
        // path("${all_qtl_results}_all_mpr/top_qtl_results_all.txt"), emit: top_qtl_results_all_all_mpr
        // path("${all_qtl_results}_qtls")
    script:
        all_qtl_results = "${condition}".replaceFirst(/.*\.tsv$/, '')

        // matcher = ("${condition}" =~ /.*?__(.*?)__(.*?)\.tsv$/)
        group0 = "${condition}".split('__')[0]
        group1 =  "${condition}".split('__')[1]
        // group1 = matcher[0][1] // Extracts 'Mono_all'
        group2 = "${condition}".split('__')[2].replaceFirst(/\.tsv/, '')

        """
            mkdir results_${group1}
            echo "${condition}"
            minimal_postprocess.py -id ./ -od results_${group1} -sfo -tfb 
            minimal_postprocess.py -id ./ -od results_${group1} -sfo -mrp 0.05 
        """
}

process TEST{
    tag { condition }
    input:
        each chunking_range
        tuple(val(condition),path(phenotypeFile),path(covariateFile))
        each path(genotypeFile)
        each path(annotationFile)

    output:
        path("out.txt"), emit: filtered_vcf
    script:
        
        """
        echo "${chunking_range}">out.txt
        multiCorrect.R ${}
        """
}


process MULTIPLE_TESTING_CORRECTION{
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    
        
    input:
        path(limix_qtl_path)


    output:
        path("${limix_qtl_path}/qtl_results_all_FDR*"), emit: filtered_vcf
    script:
        
        """
            multiCorrect.R ${limix_qtl_path}
        """
}

process LIMIX{

    tag { "${condition} ${annotationFile}" }
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.limix_container}"
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        tuple(val(condition),path(phenotypeFile),path(covariateFile),path(annotationFile),path(individual2sample_filename),path(genotypeFile),path(kinship_path))
        
    output:
        tuple val("${condition}__${covariateFile}"), path("${condition}_snp_metadata*"),path("${condition}_qtl_results_*"),path("${condition}_feature_metadata_*") , emit: qtl_data optional true
    script:
        numberOfPermutations = params.LIMIX.numberOfPermutations
        minorAlleleFrequency = params.maf
        hwe = params.LIMIX.hwe
        callRate = params.LIMIX.callRate
        windowSize = params.windowSize 
        blockSize = params.LIMIX.blockSize

        outputFolder='./'
        chunk_number = "${annotationFile}".replaceFirst(/.*__(\d+)\.tsv$/, '$1')

        if (params.genotypes.use_gt_dosage) {
            genotypeFile2 = "--bgen ${genotypeFile}"
        }else{
            genotypeFile2 = "--plink ${genotypeFile}/plink_genotypes "
        }


        """
            export NUMBA_CACHE_DIR=\$PWD
            export MPLCONFIGDIR=\$PWD
            export HOME=\$PWD
            cut -f 1,2 -d \$'\\t' ${individual2sample_filename} > data_no_sample_category.txt
            run_limix_QTL_analysis.py ${genotypeFile2} -af ${annotationFile} -pf ${phenotypeFile} -cf ${covariateFile} -od ${outputFolder} -smf data_no_sample_category.txt -rf ${kinship_path} -np ${numberOfPermutations} -maf ${minorAlleleFrequency} -hwe ${hwe} -cr ${callRate} -c -gm standardize -w ${windowSize} --block_size ${blockSize}
            mv snp_metadata_all.txt ${condition}_snp_metadata_${chunk_number}.txt || echo 'not available'
            mv qtl_results_all.h5 ${condition}_qtl_results_${chunk_number}.h5 || echo 'not available'
            mv feature_metadata_all.txt ${condition}_feature_metadata_${chunk_number}.txt || echo 'not available'
        """
    
}

workflow LIMIX_eqtls{
    take:
        filtered_pheno_channel
        plink_genotype
        genome_annotation
        kinship_file
    main:


        CHUNK_GENOME(genome_annotation,filtered_pheno_channel,params.chunkSize)

        chunking_channel=CHUNK_GENOME.out.filtered_chunking_file
        

        result = chunking_channel.flatMap { item ->
            def (condition,phenotype_file,phenotype_pcs,chunging_file,mapping_file) = item
            if (!(chunging_file instanceof Collection)) {
                chunging_file = [chunging_file] // Wrap single value in a list
            }
            return chunging_file.collect { [condition,phenotype_file,phenotype_pcs,it,mapping_file] }
        }
        // condition.splitCsv(header: true, sep: "\t").map{row ->  tuple(row.Range,row.condition,row.phenotypeFile,row.covars,row.anotation_file,row.genotype_phenotype_file)}.set{chunking_channel}
        chunking_channel = result.combine(plink_genotype)
        chunking_channel = chunking_channel.combine(kinship_file)
        // chunking_channel.subscribe { println "chunking_channeloutput_s2 dist: $it" }
        LIMIX(chunking_channel) // These are then passed to limix model which is either run in cis or trans mode.
        inp_ch2 = LIMIX.out.qtl_data.groupTuple(by: 0)
        // COLLECT_RESULTS(inp_ch2) // Results are then collected.

        AGGREGATE_QTL_RESULTS(inp_ch2) // QTL results are then aggregated.
        
        MULTIPLE_TESTING_CORRECTION(AGGREGATE_QTL_RESULTS.out.limix_qtl_path) // Multiple testing is performed.

}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/chunk_genome/main.nf =====
process CHUNK_GENOME{
    tag "$condition, $nr_phenotype_pcs"
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        each path(genome_annotation)
        tuple path(phenotype_pcs),val(condition),path(mapping_file),path(phenotype_file)

    output:
        // path("Chunging_file.tsv"), emit: chunking_file_path
        // path('annotation_file_processed.tsv'), emit: annotation_file_processed
        tuple val(condition),path("Chunging_file.tsv"),path('annotation_file_processed.tsv'),path(phenotype_file), path(phenotype_pcs) , emit: filtered_chunking_file
        path('limix_chunking.tsv'), emit: limix_condition_chunking
    script:
        nr_phenotype_pcs = phenotype_pcs.getSimpleName()
        """
            generate_chunking_file.py --genome_annotation ${genome_annotation} --chunk_size ${params.chunkSize} --phenotype_file ${phenotype_file} --covar_file ${phenotype_pcs} --condition ${condition}  --genotype_phenotype_file ${mapping_file} --gtf_gene_identifier ${params.gtf_gene_identifier}
        """
    
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/subset_genotype/main.nf =====

process SUBSET_GENOTYPE {
    tag "${samplename}.${sample_subset_file}"
    label 'process_medium'
    // publishDir "${params.outdir}/subset_genotype/", mode: "${params.copy_mode}", pattern: "${samplename}.${sample_subset_file}.subset.vcf.gz"
    
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }


    input:
        path(donor_vcf)
        val(file__reduced_dims)


    output:
    
        path("${samplename}.subset.vcf.gz"), emit: samplename_subsetvcf

    script:
        file__reduced_dims2 = file__reduced_dims.unique().join(",")
        samplename='subset'
        // sample_subset_file = donor_vcf.getSimpleName()
        // sample_names = file__reduced_dims.toString()
        """ 
            bcftools view ${donor_vcf} -s ${file__reduced_dims2} --force-samples -Oz -o ${samplename}.subset.vcf.gz --threads ${task.cpus}
        """
}
-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/plink_convert/main.nf =====
process BGEN_CONVERT{
    // Converts VCF to PLINK format, makes bed/bim/fam if use_gt_dosage param is false
    // otherwise makes pgen/psam/pvar with dosages

    scratch false      // use tmp directory
    label 'process_medium'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"

    publishDir  path: "${params.outdir}/genotypes",
                mode: "${params.copy_mode}",
                overwrite: "true"        
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        path(file__vcf)
    output:
        path("genotypes_bgen.bgen"), emit: plink_path

    script:
        if ("${file__vcf}".contains(".vcf")) {
            ext1 = "--vcf"
        } else {
            ext1 = "--bcf"
        }


        """
            plink2 ${ext1} ${file__vcf} --make-pgen --out temp
            plink2 --pfile temp --export bgen-1.2 --out genotypes_bgen
        """    
}

process PLINK_CONVERT{
    
    // Converts VCF to PLINK format, makes bed/bim/fam if use_gt_dosage param is false
    // otherwise makes pgen/psam/pvar with dosages

    scratch false      // use tmp directory
    label 'process_medium'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"

    publishDir  path: "${params.outdir}/genotypes",
                mode: "${params.copy_mode}",
                overwrite: "true"        
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        path(file__vcf)
    output:
        path("plink_genotypes_bed"), emit: plink_path
        tuple path("plink_genotypes_bed/plink_genotypes.bim"),path("plink_genotypes_bed/plink_genotypes.bed"),path("plink_genotypes_bed/plink_genotypes.fam"), emit: bim_bed_fam

    script:
        if ("${file__vcf}".contains(".vcf")) {
            ext1 = "--vcf"
        } else {
            ext1 = "--bcf"
        }


        """
            mkdir plink_genotypes_bed
            plink2 ${ext1} ${file__vcf} --make-bed ${params.plink2_filters} --hwe ${params.hwe} --out plink_genotypes_bed/plink_genotypes

        """
    
}



process PGEN_CONVERT{
    
    // Converts VCF to PLINK format, makes bed/bim/fam if use_gt_dosage param is false
    // otherwise makes pgen/psam/pvar with dosages

    scratch false      // use tmp directory
    label 'process_medium'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"

    publishDir  path: "${params.outdir}/genotypes",
                mode: "${params.copy_mode}",
                overwrite: "true"        
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        path(file__vcf)
    output:
        path("plink_genotypes_pgen"), emit: plink_path

    script:

        if ("${file__vcf}".contains(".vcf")) {
            ext1 = "--vcf ${file__vcf} 'dosage=DS' "
        }  else if ("${file__vcf}"=='plink_genotypes_bed') {
            ext1 = "--bfile ${file__vcf}/plink_genotypes"
        }else {
            ext1 = "--bcf ${file__vcf} 'dosage=DS'"
        }

        """
            mkdir plink_genotypes_pgen
            plink2 ${ext1} --max-alleles 2 --make-pgen ${params.plink2_filters} --hwe ${params.hwe} --out plink_genotypes_pgen/plink_genotypes
        """
    
    
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/preprocess_sample_mapping/main.nf =====
process PREPROCESS_SAMPLE_MAPPING{
    
    // Calulates bbknn neighbors and saves UMAPS of these
    // ------------------------------------------------------------------------
    //tag { output_dir }
    //cache false        // cache results from run
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        path(genotype_phenotype)
    output:
        path("genotype_phenotype.tsv") , emit: genotype_phenotype
    script:
        """
        
          genotype_phenotype_preprocess.py --genotype_phenotype ${genotype_phenotype}
        """
    
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/tensorqtl/processes.nf =====
tensor_label = params.utilise_gpu ? 'gpu' : "process_medium"   

process TENSORQTL {  
    label "${tensor_label}"
    tag "$condition, $interaction, $nr_phenotype_pcs"
    // cache false
    
    publishDir  path: "${params.outdir}/TensorQTL_eQTLS/${condition}/",
                overwrite: "true"
  

  if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
    container "${params.eqtl_container}"
  } else {
    container "${params.eqtl_docker}"
  }
  

  input:
    tuple(
        val(condition),
        path(aggrnorm_counts_bed),
        path(covariates_tsv),
        val(nr_phenotype_pcs),
        path(interaction_file)
    )
    each path(plink_files_prefix)
    val(skip_nominal)
    val(preprocess_bed)
    val(skip_gwas)
    val(map_independent_qtls)

  output:
    tuple val(condition), val(interaction), path("${outpath}"), emit: pc_qtls_path

  script:
    // If a file with interaction terms is provided, use the interaction script otherwise use the standard script   
    if ("${interaction_file}" != 'fake_file.fq') {
      tensor_qtl_script = "tensorqtl_analyse_interaction.py -inter ${interaction_file} --interaction_maf ${params.TensorQTL.interaction_maf}"
      inter_name = file(interaction_file).baseName
      interaction = "${inter_name}"
      outpath = "${nr_phenotype_pcs}/interaction_output/${inter_name}"
      chrom_to_map_trans = ""
    } else {
      tensor_qtl_script = "tensorqtl_analyse.py -nperm ${params.numberOfPermutations}"
      outpath = "${nr_phenotype_pcs}/base_output/base"
      interaction = 'base'
      
      if (params.TensorQTL.chrom_to_map_trans == ''){
        chrom_to_map_trans = ""
      }else{
        chrom_to_map_trans = "--chrom_to_map_trans ${params.TensorQTL.chrom_to_map_trans} "
      }
    }

    if (skip_gwas){
      chrom_to_map_trans = ""
    }

    if (skip_nominal) {
      map_nominal_flag = ""
    } else {
      map_nominal_flag = "--map_nominal"
    }
    if (map_independent_qtls==true) {
      map_independent_qtls = "--map_independent_qtls"
    } else {
      map_independent_qtls = ""
    }

    if (preprocess_bed) {
      preprocess_bed = "bedtools sort -i ${aggrnorm_counts_bed} -header > Expression_Data.sorted.bed; sed -i 's/^chr//' Expression_Data.sorted.bed"
    } else {
      preprocess_bed = ""
    }

    if (params.genotypes.use_gt_dosage) {
      dosage = "--dosage"
    }else{
      dosage = ""
    }
    """
      plink_dir="${plink_files_prefix}"
      base_name=""
      if ls "\$plink_dir"/*.psam 1> /dev/null 2>&1; then
          base_name=\$(basename \$(ls "\$plink_dir"/*.psam | head -n 1) .psam)
          pgen_or_bed="--pfile"
      elif ls "\$plink_dir"/*.bed 1> /dev/null 2>&1; then
          base_name=\$(basename \$(ls "\$plink_dir"/*.bed | head -n 1) .bed)
          pgen_or_bed="--bfile"
      else
          echo "No .psam or .bed file found in \$plink_dir"
          exit 1
      fi
      echo " Detected base name: \$base_name"
      echo "Using mode: \$pgen_or_bed"

      echo ${map_independent_qtls}
      ${preprocess_bed}
      ${tensor_qtl_script} --plink_prefix_path "\$plink_dir/\$base_name" --expression_bed Expression_Data.sorted.bed --covariates_file ${covariates_tsv} -window ${params.windowSize} ${dosage} --maf ${params.maf} --outdir ${outpath} ${map_nominal_flag} ${chrom_to_map_trans} ${map_independent_qtls}
      cd ${outpath} && ln ../../../${covariates_tsv} ./ && ln ../../../Expression_Data.sorted.bed ./ && ln ../../../${interaction_file} ./
    """
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/tensorqtl/main.nf =====
tensor_label = params.utilise_gpu ? 'gpu' : "process_medium"   

include { TENSORQTL as TENSORQTL } from './processes.nf'
include { TENSORQTL as TENSORQTL_OPTIM } from './processes.nf'

// PREP_OPTIMISE_PCS process to create symlinks
process PREP_OPTIMISE_PCS {
    label 'process_low'
    tag "$condition, $interaction"
    input:
      tuple val(condition), val(interaction), val(paths)
    output:
      tuple val(condition), val(interaction), path("${condition}_${interaction}_symlink")

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }

    script:
    paths_str = paths.join(" ")
    """
    mkdir ${condition}_${interaction}_symlink
    cd ${condition}_${interaction}_symlink
    for path in ${paths_str}; do
         unique_name=\$(echo \$path | awk -F/ '{print \$(NF-2)"__"\$(NF-1)"__"\$NF}')
        ln -s \$path \$unique_name || echo 'already linked'
    done
    """
}

process OPTIMISE_PCS{
     
    // Choose the best eQTL results based on most eGenes found over a number of PCs
    // ------------------------------------------------------------------------
    tag "$condition, $interaction"
    scratch false      // use tmp directory
    label 'process_low'
    errorStrategy 'ignore'


    publishDir  path: "${params.outdir}/TensorQTL_eQTLS/${condition}/",
                mode: "${params.copy_mode}",
                overwrite: "true"

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        tuple(val(condition),val(interaction),path(eqtl_dir))
    output:
        path("${outpath}/optimise_nPCs-FDR${alpha_text}.pdf"), emit: optimise_nPCs_plot
        path("${outpath}/optimise_nPCs-FDR${alpha_text}.txt"), emit: optimise_nPCs
        tuple val(condition), path("${tensor_input_path}/Expression_Data.sorted.bed"), path("${tensor_input_path}/Covariates.tsv"), val('OPTIM_pcs'), path("${tensor_input_path}/${interaction_file}"), emit: cis_input, optional: true
        tuple val(condition), path("${tensor_input_path}/Expression_Data.sorted.bed"), path("${tensor_input_path}/Covariates.tsv"), path("${tensor_input_path}/Cis_eqtls_qval.tsv"), emit: trans_input, optional: true
        path(outpath)
        

    script:
        sumstats_path = "${params.outdir}/TensorQTL_eQTLS/${condition}/"
        if ("${interaction}" != 'base') {
            interaction_file = "${interaction}.tsv"
            outpath_end = "interaction_output__${interaction}"
        } else {
          interaction_file = "fake_file.fq"
          outpath_end = "base_output__base"
        }
        alpha = "${params.TensorQTL.alpha}"
        alpha_text = alpha.replaceAll("\\.", "pt")
        outpath = "./OPTIM_pcs/${outpath_end}"
        tensor_input_path = "./OPTIM_input/${outpath_end}"
        """  
          mkdir -p ${outpath}
          mkdir -p ${tensor_input_path}
          tensorqtl_optimise_pcs.R ./ ${alpha} ${interaction} ${condition} ${outpath}
          var=\$(grep TRUE ${outpath}/optimise_nPCs-FDR${alpha_text}.txt | cut -f 1) && cp -r ${condition}_${interaction}_symlink/"\$var"pcs__${outpath_end}/* ${tensor_input_path}
          echo \${var} >> ${outpath}/optim_pcs.txt
        """
}

process TRANS_BY_CIS {
    label "${tensor_label}"
    tag "$condition"
    
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
      container "${params.eqtl_container}"
    } else {
      container "${params.eqtl_docker}"
    }

    publishDir  path: "${params.outdir}/TensorQTL_eQTLS/${condition}/OPTIM_pcs",
                mode: "${params.copy_mode}",
                overwrite: "true"

    input:

        tuple(
          val(condition),
          path(phenotype_file),
          path(covariates),
          path(cis_eqtls_qval)
        )
        each path(plink_files_prefix) 

    output:
        //path("${outpath}/trans-by-cis_bonf_fdr.tsv", emit: trans_res, optional: true)
        path("trans-by-cis_bonf_fdr.tsv", emit: trans_res, optional: true)
        path("trans-by-cis_all.tsv.gz", emit: trans_res_all, optional:true)

    script:
      // Use dosage?
      if (params.genotypes.use_gt_dosage) {
        dosage = "--dosage"
      }else{
        dosage = ""
      }
      if (params.TensorQTL.trans_by_cis_variant_list !='') {
        command_subset_var_list = "grep -P 'variant_id\tcondition_name|${condition}' ${params.TensorQTL.trans_by_cis_variant_list} > var_list.tsv"
        variant_list = "--variant_list var_list.tsv"
      }else{
		command_subset_var_list = ""
        variant_list = ""
      }

      """
      plink_dir="${plink_files_prefix}"
      base_name=""
      if ls "\$plink_dir"/*.psam 1> /dev/null 2>&1; then
          base_name=\$(basename \$(ls "\$plink_dir"/*.psam | head -n 1) .psam)
          pgen_or_bed="--pfile"
      elif ls "\$plink_dir"/*.bed 1> /dev/null 2>&1; then
          base_name=\$(basename \$(ls "\$plink_dir"/*.bed | head -n 1) .bed)
          pgen_or_bed="--bfile"
      else
          echo "No .psam or .bed file found in \$plink_dir"
          exit 1
      fi
      echo " Detected base name: \$base_name"
      echo "Using mode: \$pgen_or_bed"

	    ${command_subset_var_list}
      tensor_analyse_trans_by_cis.py \
        --covariates_file ${covariates} \
        --phenotype_file ${phenotype_file} \
        --plink_prefix_path "\$plink_dir/\$base_name" \
        --outdir "./" \
        ${dosage} \
        --maf ${params.maf} \
        --cis_qval_results ${cis_eqtls_qval} \
        --alpha ${params.TensorQTL.alpha} \
        --window ${params.windowSize} \
        ${variant_list}
        
      """

      
}

process TRANS_OF_CIS {
    label "process_high_memory"
    tag "$condition"
    
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
      container "${params.eqtl_container}"
    } else {
      container "${params.eqtl_docker}"
    }

    publishDir  path: "${params.outdir}/TensorQTL_eQTLS/${condition}/TRANS_BY_CIS",
                mode: "${params.copy_mode}",
                overwrite: "true"

    input:
        tuple(
          val(condition),
          path(phenotype_file),
          path(covariates),
          path(cis_eqtls_qval)
        )
        each path(plink_files_prefix) 

    output:
        //path("${outpath}/trans-by-cis_bonf_fdr.tsv", emit: trans_res, optional: true)
        path("trans-of-cis_all.tsv", emit: trans_res, optional: true)
        path("trans-of-cis_filt.tsv", emit: trans_res_filt, optional: true)

    script:
      // Use dosage?
      if (params.genotypes.use_gt_dosage) {
        dosage = "--dosage"
      }else{
        dosage = ""
      }

      alpha = "${params.TensorQTL.alpha}"

      """
      plink_dir="${plink_files_prefix}"
      base_name=""
      if ls "\$plink_dir"/*.psam 1> /dev/null 2>&1; then
          base_name=\$(basename \$(ls "\$plink_dir"/*.psam | head -n 1) .psam)
          pgen_or_bed="--pfile"
      elif ls "\$plink_dir"/*.bed 1> /dev/null 2>&1; then
          base_name=\$(basename \$(ls "\$plink_dir"/*.bed | head -n 1) .bed)
          pgen_or_bed="--bfile"
      else
          echo "No .psam or .bed file found in \$plink_dir"
          exit 1
      fi
      echo " Detected base name: \$base_name"
      echo "Using mode: \$pgen_or_bed"

      tensor_analyse_trans_of_cis.py \
        --covariates_file ${covariates} \
        --phenotype_file ${phenotype_file} \
        --plink_prefix_path "\$plink_dir/\$base_name" \
        --outdir "./" \
        --dosage ${dosage} \
        --maf ${params.maf}  \
        --cis_qval_results ${cis_eqtls_qval} \
        --alpha ${alpha} \
        --window ${params.windowSize} \
        --pval_threshold ${params.TensorQTL.trans_by_cis_pval_threshold}
      """
      //cp trans-by-cis_bonf_fdr.tsv ${outpath}
      //"""
}

process SPLIT_INTERACTIONS {
    label 'process_small'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
      container "${params.eqtl_container}"
    } else {
      container "${params.eqtl_docker}"
    }

    input:
        path multi_interaction_file
    output:
        path("interaction_files/*.tsv", emit: interactions_files)
    script:
        """
        mkdir -p interaction_files
        num_columns=\$(head -1 ${multi_interaction_file} | awk -F '\\t' '{print NF}')
        if [ "\$num_columns" -eq 2 ]; then
            # Two columns, copy the file as is, retaining its original name
            cp ${multi_interaction_file} interaction_files/
        elif [ "\$num_columns" -ge 3 ]; then
            # Three or more columns, run the Python script to split
            split_interactions.py -i ${multi_interaction_file} -o interaction_files
        else
            echo "Error: Input file must have at least two columns."
            exit 1
        fi
        """
}


process RUN_GSEA {
    // Run fGSEA for each interaction result
    // ------------------------------------------------------------------------
    
    label "process_medium"
    tag "$condition, $interaction"
    // cache false

    publishDir  path: "${params.outdir}/TensorQTL_eQTLS/${condition}/OPTIM_pcs/interaction_output/${interaction}/fgsea",
                overwrite: "true"

    input:
        tuple(
          val(condition), 
          val(interaction), 
          path(sumstats_dir)
        )

    output:
        tuple(
            val(condition),
            val(interaction),
            path("${outfile}-gsea_results.tsv.gz"),
            val(outdir),
            emit: gsea_results
        )

    script:
        outdir = "./fgsea"
        sumstats_path = "${sumstats_dir}/cis_inter1.cis_qtl_top_assoc.txt.gz"
        outfile = "${condition}__${interaction}"
        """
        fgsea_ieQTLs.R \
            --iegenes ${sumstats_path} \
            --ranking_var b_gi \
            --eps 0 \
            --unsigned_ranking \
            --gsets_gene_matrix ${projectDir}/assets/data/gene_set_genes.tsv.gz \
            --gsets_info_file ${projectDir}/assets/data/gene_set_info.tsv.gz \
            --database c2.cp.reactome \
            --n_cores ${task.cpus} \
            --output_file ${outfile} \
            --verbose 
        """
}


workflow TENSORQTL_eqtls{
    take:
        condition_bed
        plink_genotype
        
    main:

     
      if (params.TensorQTL.interaction_file != '') {
          SPLIT_INTERACTIONS(params.TensorQTL.interaction_file)
          interaction_files = SPLIT_INTERACTIONS.out.interactions_files.flatten()
          condition_bed = condition_bed.combine(interaction_files)
          
      } else {
          interaction_files = Channel.of("$projectDir/assets/fake_file.fq")
          condition_bed = condition_bed.map { cond_bed_item ->
              cond_bed_item + ["$projectDir/assets/fake_file.fq"]
          }
      }


      // if (params.TensorQTL.aggregation_subentry != '') {
      //     log.info("------- Analysing ${params.SAIGE.aggregation_subentry} celltypes ------- ")
      //     // Split the aggregation_subentry parameter into a list of patterns
      //     valid_files = phenotype_file.filter { file ->
      //         params.SAIGE.aggregation_subentry.split(',').any { pattern -> "${file}".contains("__${pattern}__") }
      //     }
      // } else {
      //     log.info('------- Analysing all celltypes ------- ')
      //     valid_files = phenotype_file
      // }

      // dMean__CD4_Naive_all, /lustre/scratch127/humgen/teams/hgi/mo11/tmp_projects127/sle_project/8.eqtl_analysis/work/6b/2a35dfd61dcea16e2a43d45eaffd0d/Expression_Data.bed.gz, /lustre/scratch127/humgen/teams/hgi/mo11/tmp_projects127/sle_project/8.eqtl_analysis/work/6b/2a35dfd61dcea16e2a43d45eaffd0d/Covariates.tsv, 20pcs, /software/hgi/pipelines/QTLight/QTLight_v1.41/assets/fake_file.fq
      // plink_genotype.subscribe { println "TENSORQTL dist: $it" }
      TENSORQTL(
          condition_bed,
          plink_genotype,
          params.TensorQTL.optimise_pcs,
          true,
          true,
          false
      )

      if (params.TensorQTL.optimise_pcs){
          // TENSORQTL.out.pc_qtls_path.view()
          // Make sure all input files are available before running the optimisation
          // Fix the format of the output from TENSORQTL
          prep_optim_pc_channel = TENSORQTL.out.pc_qtls_path.groupTuple(by: [0,1]).map { key1, key2, values -> [key1, key2, values.flatten()]}
          // Create symlinks to the output files
          PREP_OPTIMISE_PCS(prep_optim_pc_channel)
          // Run the optimisation to get the eQTL output with the most eGenes
          OPTIMISE_PCS(PREP_OPTIMISE_PCS.out)

          TENSORQTL_OPTIM(
            OPTIMISE_PCS.out.cis_input,
            plink_genotype,
            false,
            false,
            false,
            params.TensorQTL.map_independent_qtls
          )

          if (params.TensorQTL.interaction_file != '' && params.TensorQTL.run_gsea){
            RUN_GSEA(TENSORQTL_OPTIM.out.pc_qtls_path)
          }

          if(params.TensorQTL.trans_by_cis){
            log.info 'Running trans-by-cis analysis on optimum nPCs'
            TRANS_BY_CIS(
              OPTIMISE_PCS.out.trans_input,
              plink_genotype
            )
          }
          
          if(params.TensorQTL.trans_of_cis){
            log.info 'Running trans-of-cis analysis on optimum nPCs'
            TRANS_OF_CIS(
              OPTIMISE_PCS.out.trans_input,
              plink_genotype
            )
          }
  }
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/kinship_calculation/main.nf =====

process KINSHIP_CALCULATION {
    tag "${samplename}.${sample_subset_file}"
    label 'process_medium'
    publishDir "${params.outdir}/subset_genotype/", mode: "${params.copy_mode}", pattern: "${samplename}.${sample_subset_file}.subset.vcf.gz"
    
    
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }


    input:
        path(plink_path)

    output:
    
        path("kinship_matrix.tsv"), emit: kinship_matrix

    script:
        if (params.genotypes.use_gt_dosage) {
            pgen_or_bed = "--pfile"
        }else{
            pgen_or_bed = "--bfile"
        }
        // pgen_or_bed = "--bfile" //ATM only bed is used in LIMIX
        """
            plink2 --freq counts ${pgen_or_bed} ${plink_path}/plink_genotypes --out tmp_gt_plink_freq
            plink2 --make-rel square --read-freq tmp_gt_plink_freq.acount ${pgen_or_bed} ${plink_path}/plink_genotypes
            generate_kinship.py
        """
}
-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/prepere_exp_bed/main.nf =====

process PREPERE_EXP_BED {
  label 'process_medium'
  tag "$condition"
  if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
      container "${params.eqtl_container}"
      // container "/software/hgi/containers/yascp/yascp.cog.sanger.ac.uk-public-singularity_images-eqtl_19_09_2023.img.img"
      
  } else {
      container "${params.eqtl_docker}"
  }


  input:
    tuple(path(phenotype_pcs),val(condition),path(mapping_file),path(expression_file))
    each path(annotation_file)

  output:
    tuple(val("${condition}__${phenotype_pcs}"),path("Expression_Data.bed.gz"), emit: exp_bed)

  script:
    if(params.covariates.extra_covariates_file==''){
      sample_covar =''
    }else{
      sample_covar ="--sample_covariates ${params.covariates.extra_covariates_file}"
    }


    if ("${params.chromosomes_to_test}"!=''){
        chromosomes_as_string = params.chromosomes_to_test.join(',')
        cond2 = " --chr ${chromosomes_as_string}"
    }else{
        cond2 = " "
    }

    """
      echo ${condition}
      prepere_bed.py --annotation_file ${annotation_file} --mapping_file ${mapping_file} --expression_file ${expression_file} --position ${params.position} --gtf_gene_identifier ${params.gtf_gene_identifier} --gtf_type ${params.gtf_type} ${cond2} 
    """
}

process PREPERE_COVARIATES {
  label 'process_medium'
  tag "$condition, $nr_phenotype_pcs"
  if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
      container "${params.eqtl_container}"
  } else {
      container "${params.eqtl_docker}"
  }

  input:
    tuple(path(phenotype_pcs),val(condition),path(mapping_file),path(expression_file))
    each path(genotype_pcs)

  output:
    tuple(val("${condition}__${phenotype_pcs}"),path('Covariates.tsv'), val(nr_phenotype_pcs), emit: exp_bed)

  script:
    nr_phenotype_pcs = phenotype_pcs.getSimpleName()
    if(params.covariates.extra_covariates_file==''){
      sample_covar =''
    }else{
      sample_covar ="--sample_covariates ${params.covariates.extra_covariates_file}"
    }

    """
      echo ${condition}
      prepere_covariates_file.py --genotype_pcs ${genotype_pcs} --phenotype_pcs ${phenotype_pcs} ${sample_covar} --sample_mapping ${mapping_file} --nr_gPCs ${params.covariates.nr_genotype_pcs}
    """
}

process PREP_SAIGE_COVS {
  label 'process_medium'
  tag "$condition"
  if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
      container "${params.eqtl_container}"
  } else {
      container "${params.eqtl_docker}"
  }

  input:
    path(genotype_pcs)
    path(extra_covariates_file)
  output:
    path('G_E_Covariates.tsv')

  script:

    if(params.covariates.extra_covariates_file==''){
      sample_covar =''
    }else{
      sample_covar ="--sample_covariates ${extra_covariates_file}"
    }
    // subset to number of gPCs required and also append the extra covariates that may be provided. 
    """
      prepere_covariates_file_SAIGE.py --genotype_pcs ${genotype_pcs} ${sample_covar} --nr_gPCs ${params.covariates.nr_genotype_pcs}
    """
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/preprocess_genotypes/main.nf =====
process PREPROCESS_GENOTYPES{
    
    // Subsets genotypes
    // ------------------------------------------------------------------------
    //tag { output_dir }
    //cache false        // cache results from run
    scratch false      // use tmp directory
    label 'process_medium'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    publishDir  path: "${params.outdir}/genotypes",
                mode: "${params.copy_mode}",
                overwrite: "true"       
                
    input:
        path(file__vcf)
    output:
        path("filtered_vcf.vcf.gz") , emit: filtered_vcf
    script:
        """
            bcftools index ${file__vcf} || echo 'exists'
            bcftools view ${params.bcftools_filters} ${file__vcf} -Oz -o filtered_vcf.vcf.gz
            #bcftools sort filtered_vcf.vcf.gz -Oz -o filtered_vcf2.vcf.gz
            #rm filtered_vcf.vcf.gz
        """
    
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/covar_processing/main.nf =====
process SUBSET_PCS{

     
    // Normalise expression data and perform PCA
    // ------------------------------------------------------------------------
    tag { condition }
    //cache false        // cache results from run
    scratch false      // use tmp directory
    label 'process_medium'

    publishDir  path: "${params.outdir}/norm_data/${condition}",
                mode: "${params.copy_mode}",
                overwrite: "true"

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        tuple(val(condition),path(mappings_handeling_repeats),path(normalised_phenotype),path(all__pcs),val(pc1))

    output:
        tuple val(condition),path(mappings_handeling_repeats),path(normalised_phenotype),path("${pc1}pcs.tsv"), emit: for_bed optional true

    script:
    // Here we split the pcs based on the input file. This is removed from normalise part to avoid normalisation multiple times if new pcs asre added.
        """  
            PC_subset.R ${all__pcs} ${pc1}
        """
    


}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/aggregate_UMI_counts/main.nf =====
process SPLIT_AGGREGATION_ADATA {
    // label 'process_medium'
    memory { 
            sizeInGB = adata.size() / 1e9 * 0.5 * task.attempt
            return (sizeInGB ).toString() + 'GB' 
        }
      
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

  input:
    path(adata) // lists input files per donor
    val(agg_columns)

  output:
    path("*__split.h5ad", emit:split_phenotypes)
  script:
    if ("${params.aggregation_subentry}"==''){
        cond1 = ""
    }else{
        cond1 = " --condition '${params.aggregation_subentry}' "
    }
  """
    split_adata_per_condition.py --agg_columns '${agg_columns}' -h5ad ${adata} ${cond1}
  """
}


process ORGANISE_AGGREGATED_FILES{

  publishDir  path: "${params.outdir}/aggregated_counts/${sanitized_columns}",mode: "${params.copy_mode}",
              overwrite: "true"
    label 'process_medium'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }
    memory { 
            sizeInGB = adata.size() / 1e9 * 0.3 * task.attempt
            return (sizeInGB ).toString() + 'GB' 
        }
  input:
    tuple val(sanitized_columns), path(phenotype_files),  path(genotype_phenotype_mapping)

  output:
    path("clean_table.tsv", emit:phenotype_files_tsv) optional true


  script:
    
    """
      organise_channels.py -files "${phenotype_files}" -files2 "${genotype_phenotype_mapping}"
    """


}

process AGGREGATE_UMI_COUNTS {
  publishDir  path: "${params.outdir}/aggregated_counts/${sanitized_columns}",mode: "${params.copy_mode}",
              overwrite: "true"
    label 'process_medium'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }
    memory { 
            sizeInGB = adata.size() / 1e9 * 0.3 * task.attempt
            return (sizeInGB ).toString() + 'GB' 
        }
  input:
    path(adata) // lists input files per donor
    val(agg_columns)
    val(gt_id_column)
    val(sample_column)
    val(n_cells_min)
    val(n_donors_min)

  output:
    path("*phenotype_file.tsv", emit:phenotype_file) optional true
    path("*genotype_phenotype_mapping.tsv", emit:genotype_phenotype_mapping) optional true
    tuple val(sanitized_columns), path("*phenotype_file.tsv"),  path("*genotype_phenotype_mapping.tsv"), emit:phenotype_genotype_file optional true
    path('*.tsv') optional true

  script:
    sanitized_columns = adata.getName().replaceAll(/[^a-zA-Z0-9]/, '_').replaceAll(/\.h5ad$/, '')
    """
      echo ${sanitized_columns}
      aggregate_sc_data.py --agg_columns '${agg_columns}' --gt_id_column '${gt_id_column}' --sample_column '${sample_column}' --n_cells ${n_cells_min} -n_individ ${n_donors_min} -h5ad ${adata} --method ${params.aggregation_method} --cell_percentage_threshold ${params.cell_percentage_threshold}
    """
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/normalise_anndata/main.nf =====


process NORMALISE_ANNDATA {
    publishDir  path: "${params.outdir}/normalise_anndata/${sanitized_columns}",mode: "${params.copy_mode}",
                overwrite: "true"
    if ("${params.dMean_norm_method}"=='NONE'){
      label 'process_low'
    }else{
        memory { 
            sizeInGB = adata.size() / 1e9 
            return (sizeInGB * 6 * task.attempt).toString() + 'GB' 
        }
    }

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
      each path(adata)

    output:
      path("nAD_*.h*", emit:adata) optional true
    script:
      sanitized_columns = adata.getName().replaceAll(/[^a-zA-Z0-9]/, '_').replaceAll(/\.h5ad$/, '')
      
      if ("${params.dMean_norm_method}"=='NONE'){
        comand="ln -s ${adata} nAD_${adata}"
      }else{
        comand="normalise_anndata.py -h5ad ${adata} --method ${params.dMean_norm_method}"
      }

      """
        ${comand}
      """
}


process REMAP_GENOTPE_ID{
    publishDir  path: "${params.outdir}/norm_data/${sanitized_columns}_${prefix}",
                overwrite: "true"
    label 'process_low'

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
      tuple val(sanitized_columns), path(phenotype_file),  path(adata_emmited_file), path(mapping_file)
    output:
      tuple val(sanitized_columns), path(phenotype_file),  path("remap_*.tsv"), emit:remap_genotype_phenotype_mapping
      path("remap_*.tsv"),emit: genotype_phenotype_mapping
    script:
      matcher = (phenotype_file =~ /^([^_]+)___/)
      prefix = matcher ? matcher[0][1] : 'all'
      """
        replace_genotype_ids.py --mappings ${mapping_file} --genotype_phenotype_mapping ${adata_emmited_file}
      """


}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/genotype_pc_calculation/main.nf =====

process GENOTYPE_PC_CALCULATION {
    tag "${samplename}.${sample_subset_file}"
    label 'process_medium'
    publishDir "${params.outdir}/subset_genotype/", mode: "${params.copy_mode}", pattern: "${samplename}.${sample_subset_file}.subset.vcf.gz"
    
    
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        log.info 'change the docker container - this is not the right one'
        container "${params.eqtl_docker}"
    }


    input:
        path(plink_dir)

    output:
    
        path("gtpca_plink.eigenvec"), emit: gtpca_plink

    script:
        if (params.genotypes.use_gt_dosage) {
            pgen_or_bed = "--pfile"
        }else{
            pgen_or_bed = "--bfile"
        }
        """
            plink_dir="${plink_dir}"
            base_name=""

            if ls "\$plink_dir"/*.psam 1> /dev/null 2>&1; then
                base_name=\$(basename \$(ls "\$plink_dir"/*.psam | head -n 1) .psam)
                pgen_or_bed="--pfile"
            elif ls "\$plink_dir"/*.bed 1> /dev/null 2>&1; then
                base_name=\$(basename \$(ls "\$plink_dir"/*.bed | head -n 1) .bed)
                pgen_or_bed="--bfile"
            else
                echo " No .psam or .bed file found in \$plink_dir"
                exit 1
            fi

            echo "Detected base name: \$base_name"
            echo "Using mode: \$pgen_or_bed"

            n_samples=\$( [ -f "\$plink_dir/\$base_name.psam" ] && tail -n +2 "\$plink_dir/\$base_name.psam" | wc -l || wc -l < "\$plink_dir/\$base_name.fam" )
            n_pcs=\$(( n_samples < 200 ? n_samples : 200 ))
            echo "Using \$n_pcs PCs for \$n_samples samples"

            plink2 ${pgen_or_bed} "\$plink_dir/\$base_name" ${params.covariates.genotype_pc_filters} --out tmp_gt_plink_freq
            plink2 ${pgen_or_bed} "\$plink_dir/\$base_name" --extract tmp_gt_plink_freq.prune.in --freq --out tmp_gt_plink_freq
            plink2 --pca \$n_pcs --read-freq tmp_gt_plink_freq.afreq --extract tmp_gt_plink_freq.prune.in ${pgen_or_bed} "\$plink_dir/\$base_name" --out gtpca_plink
        """
}
-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/split_phenotype_data/main.nf =====
process SPLIT_PHENOTYPE_DATA{
    
    // Calulates bbknn neighbors and saves UMAPS of these
    // ------------------------------------------------------------------------
    //tag { output_dir }
    //cache false        // cache results from run

    tag{condition}
    scratch false      // use tmp directory
    label 'process_medium'

    memory { 
        sizeInGB = phenotype_file.size() / 1e9 * 0.3 * task.attempt
        return (sizeInGB ).toString() + 'GB' 
    }

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        path(annotation_file)
        path(phenotype_file)
        val(condition)
    output:
        tuple val(condition),path("*_phenotype.tsv"),path(annotation_file), emit: phenotye_file

    script:
        
        """
            split_phenotype_for_condition.py --condition '${condition}' --genome_phenotype ${annotation_file} --phenotype ${phenotype_file} 
        """
    
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/normalise_and_pca/main.nf =====
process NORMALISE_and_PCA_PHENOTYPE{
     
    // Normalise expression data and perform PCA
    // ------------------------------------------------------------------------
    tag { condition }
    //cache false        // cache results from run
    scratch false      // use tmp directory
    label 'process_medium'

    publishDir  path: "${params.outdir}/norm_data/${condition}__${prefix}",
                mode: "${params.copy_mode}",
                overwrite: "true"

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        tuple(val(condition),path(phenotype_file),path(grouping_file))
        

    output:
        tuple(val(outname),path("normalised_phenotype.tsv"), path("all__pcs.tsv") , emit: filtered_phenotype)
        tuple(val(outname),path('mappings_handeling_repeats.tsv'),path("normalised_phenotype.tsv"),path("all__pcs.tsv"), emit: for_bed)
        val(outname), emit: cond1
        path("*.pdf")
        path(phenotype_file)
        path('mappings_handeling_repeats.tsv'), emit: gen_phen_mapping
    script:
        matcher = (phenotype_file =~ /^([^_]+)___/)
        prefix = matcher ? matcher[0][1] : 'all'
        outname = "${condition}__${prefix}"
        """  
            echo ${prefix}
            echo ${outname}
            normalise_and_pca.R ${phenotype_file} ${grouping_file} ${params.filter_method} ${params.method} ${params.inverse_normal_transform} ${params.norm_method} ${params.percent_of_population_expressed} ${params.use_sample_pca}
        """
    
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/jaxqtl/main.nf =====
include {JAXQTL;JAXQTL as JAXQTL_NOMINAL} from './functions.nf'

process AGGREGATE_QTL_RESULTS{
    tag { condition }
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    
    

    publishDir  path: "${params.outdir}/JAX_eQTLS/${group0}__${group1}__${group2}/${group4}",
                mode: "${params.copy_mode}",
                overwrite: "true"

    input:
        tuple val(condition), path(full_output_list)
        
    output:
        tuple val("${group0}__${group1}__${group2}"), path("*merged_cis_scores.tsv.gz"), emit: jax_qtl_path

    script:
        all_qtl_results = "${condition}".replaceFirst(/.*\.tsv$/, '')

        group0 = "${condition}".split('__')[0]
        group4 = "${condition}".split('__')[4]
        group1 =  "${condition}".split('__')[1]
        group2 = "${condition}".split('__')[2].replaceFirst(/\.tsv/, '')

        """
            echo "${condition} ${group1} ${group2} ${group0}"
            merge_chunks.py -o ${group0}__${group1}__${group2}__${group4}__merged_cis_scores.tsv.gz
        """
}


process OPTIM_PCS{
    tag { condition }
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    
    

    publishDir  path: "${params.outdir}/JAX_eQTLS/${condition}/OPTIM_PCs",
                mode: "${params.copy_mode}",
                overwrite: "true"

    input:
        tuple val(condition), path(full_combined_lists)
        
    output:
        tuple val(condition), path("results/optimise_nPCs-FDR*_optimal_PC.txt"), emit: optimal_pc_file
        path('results/*')

    script:

        """
            echo "${full_combined_lists}"
            plot_optimal_pcs.R ./ 0.05 "${condition}" ./results
        """
}







process CHUNK_BED_FILE{
    tag "$condition, $nr_phenotype_pcs"
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
        
    } else {
        container "${params.eqtl_docker}"
    }

    input:
        tuple(
            val(condition),
            path(aggrnorm_counts_bed),
            path(covariates_tsv),
            val(nr_phenotype_pcs)
        )
        val(chunk_size)

    output:
        tuple val(condition),path(aggrnorm_counts_bed), path(covariates_tsv),val(nr_phenotype_pcs),path("gene_chunks*.tsv"), emit: chunked_bed_channel optional true

    when:
        "${condition}".contains("dSum") //Jax is supposed to work only of dSum

    script:
        """
            echo ${aggrnorm_counts_bed}
            generate_jax_chunking_file.py --chunk_size ${chunk_size} --bed_file ${aggrnorm_counts_bed} --output_prefix gene_chunks
        """
    
}


workflow JAXQTL_eqtls{
    take:
        condition_bed
        plink_genotype
        
    main:
      // 
    //   condition_bed.subscribe { println "condition_bed: $it" }

      CHUNK_BED_FILE(condition_bed,params.JAXQTL.number_of_genes_per_chunk)
      chunking_channel=CHUNK_BED_FILE.out.chunked_bed_channel
    //   chunking_channel.subscribe { println "chunking_channel: $it" }
      result = chunking_channel.flatMap { item ->
          def (condition,phenotype_file,phenotype_pcs,nr_phenotype_pcs,chunging_file) = item
          if (!(chunging_file instanceof Collection)) {
              chunging_file = [chunging_file] // Wrap single value in a list
          }
          return chunging_file.collect { [condition,phenotype_file,phenotype_pcs,nr_phenotype_pcs,it] }
      }

      JAXQTL(
          result,
          plink_genotype,
          'cis'
      )

      inp_ch2 = JAXQTL.out.qtl_data
        .groupTuple(by: 0)
        .map { cond, files -> tuple(cond, files.unique { it.toString() }) }

      // Combine results and do Qval correction

      AGGREGATE_QTL_RESULTS(inp_ch2) // QTL results are then aggregated.
      all_basic_results = AGGREGATE_QTL_RESULTS.out.jax_qtl_path
        .groupTuple(by: 0)
      // Estimate the OptimPCs
      OPTIM_PCS(all_basic_results)
      optimal_pc_file = OPTIM_PCS.out.optimal_pc_file
      optimal_pc_file
        .map { condition, file ->
            def content = file.text.trim()
            if (content) {
            return ["${condition}__${content}pcs.tsv"]
            } else {
            return null
            }
        }
        .filter { it != null } // Skip if file was empty
        .set { optimal_pc_values }

      results_for_nominal = result.combine(optimal_pc_values,by:0)

      JAXQTL_NOMINAL(
          results_for_nominal,
          plink_genotype,
          'nominal'
      )


      // Run the nominal QTLs.

}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/jaxqtl/functions.nf =====

process JAXQTL {  
    tag "$condition, $nr_phenotype_pcs"
    label "process_high_memory"
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }


    publishDir path: { "${params.outdir}/JAX_eQTLS/${group0}__${group1}__${group2}/OPTIM_PCs/${mode1}" },
            saveAs: {filename ->
                if (filename.contains("cis_score.tsv.gz")) {
                    null
                } else {
                    filename
                }
            },
            mode: "${params.copy_mode}",
            overwrite: true

    input:
        tuple(
            val(condition),
            path(aggrnorm_counts_bed),
            path(covariates_tsv),
            val(nr_phenotype_pcs),
            path(genelist)
        )
        each path(plink_files_prefix)
        val(mode1)

    output:
        tuple val("${condition}__${nr_phenotype_pcs}"), path('*cis_score.tsv.gz'), emit: qtl_data optional true
        tuple val("${condition}__${nr_phenotype_pcs}"), path('*cis_qtl_pairs.*.parquet'), emit: nominal_qtl_data optional true
        

    script:

        group0 = "${condition}".split('__')[0]
        group1 =  "${condition}".split('__')[1]
        group2 =  "${condition}".split('__')[2]
        """

            export DISABLE_PANDERA_IMPORT_WARNING=True
            plink_dir="${plink_files_prefix}"
            base_name=""
            outname=\$(basename ${genelist})
            if ls "\$plink_dir"/*.psam 1> /dev/null 2>&1; then
                base_name=\$(basename \$(ls "\$plink_dir"/*.psam | head -n 1) .psam)
                pgen_or_bed="--pfile"
            elif ls "\$plink_dir"/*.bed 1> /dev/null 2>&1; then
                base_name=\$(basename \$(ls "\$plink_dir"/*.bed | head -n 1) .bed)
                pgen_or_bed="--bfile"
            else
                echo "No .psam or .bed file found in \$plink_dir"
                exit 1
            fi
            transpose_covs.py --infile ${covariates_tsv} --outfile Covariates.fixed.tsv
            jaxqtl \
            --geno "\$plink_dir/\$base_name"  \
            --covar Covariates.fixed.tsv \
            --pheno ${aggrnorm_counts_bed} --genelist ${genelist}  \
            --model NB \
            --mode ${mode1} \
            --window ${params.windowSize} \
            --test-method score \
            --nperm ${params.numberOfPermutations} \
            --addpc 0 \
            --standardize \
            -p cpu \
            --out ${mode1}__\$outname


        """
}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/saige/main.nf =====
process CONDITIONAL_QTL {
    label 'process_tiny'

    // Finds top variant per gene and calculates up to 5 conditionally independent signals by including additional variant effects in the model

    // maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.saige_container}"
    } else {
        container "${params.saige_docker}"
    }    

    input:
        tuple val(name),path(output),path(output_rda),path(output3),path(for_conditioning),path(genome_regions),path(plink_bim), path(plink_bed), path(plink_fam),val(chr)

    script:
        if (params.SAIGE.cis_trans_mode=='cis'){
            mode="--rangestoIncludeFile=\${step2prefix}_region_file.txt"
        }else{
            mode=""
        }
    """
        echo "~~~~~~~~~~~~~~~~PERFORMING THE SECOND ROUND OF EQTL ANALYSIS~~~~~~~~~~~~~~~~~"
        #

        # Extract the minimum p-value for which there is an FDR < 0.05 in the first round
        #pval_thresh=\$(awk -F'\t' 'NR==2 {print \$13}' ${output3}_minimum_q.txt)        

        # Now perform an additional 3 rounds if we keep finding conditional 

        run_step2_tests_qtl() {
            { 
                topvariant=\$(awk -F'\t' 'NR==2 {print \$3}' ${output3}/cis_\${gene}_${chr}_minimum_q.txt)
                step2_tests_qtl.R       \
                    --bedFile=${plink_bed}      \
                    --bimFile=${plink_bim}      \
                    --famFile=${plink_fam}      \
                    --SAIGEOutputFile=output_${name}___${chr}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable}    \
                    --chrom=${chr}       \
                    --minMAF=${params.SAIGE.minMAF} \
                    --minMAC=${params.SAIGE.minMAC} \
                    --LOCO=FALSE    \
                    --varianceRatioFile=\${step1prefix}_\${variable}.varianceRatio.txt    \
                    --GMMATmodelFile=\${step1prefix}_\${variable}.rda    \
                    --SPAcutoff=${params.SAIGE.SPAcutoff} \
                    --condition=\$topvariant \
                    --markers_per_chunk=${params.SAIGE.markers_per_chunk} ${mode} 

                if [ \$? -ne 0 ]; then
                    echo "step2_tests_qtl.R command failed" >&2
                    exit 1  # Exit the script with a non-zero status
                fi

                line_count=\$(wc -l < output_${name}___${chr}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable})        
                if [ "\$line_count" -eq 1 ]; then
                    echo "File has exactly one line"
                    rm output_${name}___${chr}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable}
                else
                    echo "\${variable}" >> genes_list2.tsv
                fi
                
            } || { 
                echo 'Failed since no markers present in range'
                rm output_${name}___${chr}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable}
            }
        }


        if awk '\$2 == ${chr} {found=${chr}; exit} END {exit !found}' ${genome_regions}; then
            echo "The chromosome '${chr}' is found in the second column."

            step1prefix=${output}/nindep_100_ncell_100_lambda_2_tauIntraSample_0.5           
            step2prefix=output_${name}___${chr}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis
            mkdir -p output_${name}___${chr}
            
            cat "${genome_regions}" | while IFS= read -r gene || [ -n "\$gene" ]
            do
                echo "\$gene" | cut -f2- >> regions_cis.tsv
                variable=\$(echo "\$gene" | cut -f1)
                echo \${variable}
                chr1=\$(echo "\$gene" | cut -f2)
                
                if [ "\$chr1" -eq ${chr} ]; then
                    run_step2_tests_qtl
                else
                    echo 'Not on the correct chromosome'
                    
                fi
                rm regions_cis.tsv
            done
        else
            echo "The chromosome '${chr}' is not found in the testing range, and hence ignored."
        fi

    """
}

process CREATE_SPARSE_GRM {

    label 'process_low'

    // Specify the number of forks (10k)
    maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.saige_grm_container}"
    } else {
        container "${params.saige_grm_docker}"
    }    

    input:
        tuple path(plink_bim), path(plink_bed), path(plink_fam)
        
    output:
        // tuple val(name),path(genes_list),path("output"),emit:output
        path("sparseGRM_*.mtx"), emit: sparseGRM
        path("sparseGRM_*.sampleIDs.txt"), emit: sparseGRM_sample

    // Define the Bash script to run for each array job
    script:
    """
        createSparseGRM.R \
            --nThreads=${task.cpus} \
            --outputPrefix=sparseGRM_output \
            --numRandomMarkerforSparseKin=2000 \
            --relatednessCutoff ${params.SAIGE.relatednessCutoff} \
            --famFile ${plink_fam} \
            --bimFile ${plink_bim} \
            --bedFile ${plink_bed} 
    """  
}

process SAIGE_S1 {
    label 'process_low'

    // Specify the number of forks (10k)
    maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.saige_container}"
    } else {
        container "${params.saige_docker}"
    }    


    input:
        tuple val(name),path(genes_list),path(pheno_file),path(cov),path(plink_bim), path(plink_bed), path(plink_fam)
        each path(sparseGRM)
        each path(sparseGRM_samples)
    output:
        tuple val(name),path(genes_list),path("output"),emit:output
        path("*genes_droped_from_s1_due_to_error.tsv"), emit: dropped optional true

    // Define the Bash script to run for each array job
    script:
    """
        # Execute with the bash executable in an array (one job per gene within level)
        #// Genome wide for this we send a list of genes in chunks 
        mkdir -p output
        cat "${genes_list}" | while IFS= read -r i || [ -n "\$i" ]
        do
           {  # try
            step1_fitNULLGLMM_qtl.R \
                --useSparseGRMtoFitNULL=TRUE  \
                --sparseGRMFile ${sparseGRM} --sparseGRMSampleIDFile ${sparseGRM_samples} --relatednessCutoff ${params.SAIGE.relatednessCutoff} \
                --useGRMtoFitNULL=FALSE \
                --phenoFile=${pheno_file}	\
                --phenoCol=\$i       \
                --covarColList=\$(head -n 1 ${cov})    \
                --sampleCovarColList=\$(sed -n '2p' ${cov})      \
                --sampleIDColinphenoFile=${params.gt_id_column} \
                --traitType=count \
                --outputPrefix=./output/nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_\$i  \
                --skipVarianceRatioEstimation=FALSE  \
                --isRemoveZerosinPheno=FALSE \
                --isCovariateOffset=FALSE  \
                --isCovariateTransform=TRUE  \
                --skipModelFitting=FALSE  \
                --tol=0.00001 --traceCVcutoff 0.005 --nrun 15  \
                --famFile ${plink_fam} \
                --bimFile ${plink_bim} \
                --bedFile ${plink_bed} \
                --IsOverwriteVarianceRatioFile=TRUE ${params.SAIGE.step1_extra_flags}
            } || {
                # catch
                sed -i '/\$i/d' ${genes_list}
                echo \$i >> \${i}_genes_droped_from_s1_due_to_error.tsv
            }
        done

        cat "${genes_list}" | while IFS= read -r i || [ -n "\$i" ]
        do
            echo \$i >> ./output/gene_string.tsv
            step1prefix=./output/nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_\$i
            echo -e "\${i} \${step1prefix}.rda \${step1prefix}.varianceRatio.txt" >> ./output/step1_output_formultigenes.txt
        done
    """
}


process SAIGE_S2_CIS {
    label 'process_low'

    // Specify the number of forks (10k)
    maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.saige_container}"
    } else {
        container "${params.saige_docker}"
    }    

    input:
        tuple (val(name),path(genes_list),path(output),path(genome_regions),path(plink_bim), path(plink_bed), path(plink_fam))
        each path(sparseGRM)
        each path(sparseGRM_samples)

    output:
        tuple val("${name}"),path("genes_list2.tsv"),path("output_${name}___*"),path(output),path(genome_regions),path(plink_bim), path(plink_bed), path(plink_fam),emit:output optional true
        tuple val("${name}"),path("output_${name}___*/*___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_*"),emit:for_aggregation  optional true

    script:
        if (params.SAIGE.cis_trans_mode=='cis'){
            mode="--rangestoIncludeFile=regions_cis.tsv --chrom=\${chr1}"
        }else{
            if (params.SAIGE.trans_chr_to_test=='SAME'){
                mode="--chrom=\${chr1}"
            }else{
                mode="--chrom=${params.SAIGE.trans_chr_to_test}"
            }
            
        }
    
    """
        run_step2_tests_qtl() {
            { 
                warning_output=\$(step2_tests_qtl.R       \
                    --bedFile=${plink_bed}      \
                    --bimFile=${plink_bim}      \
                    --famFile=${plink_fam}      \
                    --sparseGRMFile ${sparseGRM} --sparseGRMSampleIDFile ${sparseGRM_samples} --relatednessCutoff=${params.SAIGE.relatednessCutoff} \
                    --SAIGEOutputFile=output_${name}___\${chr1}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable}    \
                    --minMAF=${params.SAIGE.minMAF} \
                    --minMAC=${params.SAIGE.minMAC} \
                    --LOCO=FALSE    \
                    --varianceRatioFile=\${step1prefix}_\${variable}.varianceRatio.txt    \
                    --GMMATmodelFile=\${step1prefix}_\${variable}.rda    \
                    --SPAcutoff=${params.SAIGE.SPAcutoff} \
                    --markers_per_chunk=${params.SAIGE.markers_per_chunk} ${mode}   2>&1)

                if [ \$? -ne 0 ]; then
                    echo "step2_tests_qtl.R command failed" >&2
                    return  # Skip the rest of the function and go to the next iteration
                fi

                if [[ \$? -eq 0 && "\$warning_output" != *"Input/output error"* ]]; then
                    echo "proceed"
                else
                    echo "warning exists"
                    return 1
                fi

                line_count=\$(wc -l < output_${name}___\${chr1}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable})        
                if [ "\$line_count" -eq 1 ]; then
                    echo "File has exactly one line"
                    rm output_${name}___\${chr1}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable}
                elif [ "\$line_count" -eq 0 ]; then
                    echo "File has exactly zero lines"
                    return 1
                else
                    echo "\${variable}\t\${chr1}" >> genes_list2.tsv
                fi
                
            } || { 
                echo 'Failed since no markers present in range'
                rm output_${name}___\${chr1}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis_\${variable}
            }
        }


        step1prefix=${output}/nindep_100_ncell_100_lambda_2_tauIntraSample_0.5           
        
        
        
        cat "${genome_regions}" | while IFS= read -r gene || [ -n "\$gene" ]
        do
            echo "\$gene" | cut -f2- >> regions_cis.tsv
            variable=\$(echo "\$gene" | cut -f1)
            echo \${variable}
            chr1=\$(echo "\$gene" | cut -f2)
            step2prefix=output_${name}___\${chr1}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis
            mkdir -p output_${name}___\${chr1}
            run_step2_tests_qtl
            rm regions_cis.tsv
        done


    """
}

process SAIGE_QVAL_COR {
    label 'process_low'

    // Specify the number of forks (10k)
    maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "https://yascp.cog.sanger.ac.uk/public/singularity_images/wtsihgi_nf_scrna_qc_6bb6af5-2021-12-23-3270149cf265.sif"
        //// container "/lustre/scratch123/hgi/projects/ukbb_scrna/pipelines/singularity_images/nf_qc_cluster_2.4.img"
    } else {
        container "wtsihgi/nf_scrna_qc:6bb6af5"
    }

    input:
        tuple val(name),path(genes_list),path(output),path(output_rda),path(regions),path(plink_bim), path(plink_bed), path(plink_fam)

    output:
        tuple val(name),path(genes_list),path(output),emit:output
        tuple val(name),path(output),path(output_rda),path('output3'),path('for_conditioning.csv'),path(regions),path(plink_bim), path(plink_bed), path(plink_fam), emit: for_conditioning optional true
        tuple val(name),path("output3/*_minimum_q.txt"), emit: q_out
    script:

        parts = name.split('___')
        chr = parts[-1]
        exp = parts[0]
    """
        
        
        mkdir -p output3 
        cat "${genes_list}" | while IFS= read -r gene1 || [ -n "\$gene1" ]
        do
            chr1=\$(echo "\${gene1}" | cut -f2)
            gene=\$(echo "\${gene1}" | cut -f1)
            step2prefix=output_${name}___\${chr1}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis
            qvalue_correction.py -f \${step2prefix}_\${gene} -c "13" -n "qvalues" -w "TRUE"
            mv \${step2prefix}_\${gene}_minimum_q.txt output3/cis_\${gene}_\${chr1}_minimum_q.txt

            top_q=\$(awk -F'	' 'NR==2 {print \$17}' output3/cis_\${gene}_\${chr1}_minimum_q.txt)
            threshold=${params.SAIGE.q_val_threshold_for_conditioning}
            echo "\$top_q"
            if awk -v tq="\$top_q" -v th="\$threshold" 'BEGIN {exit !(tq <= th)}'; then
                echo "Performing conditional analysis: q-value for first pass <= \$threshold"
                echo \${gene},${output_rda}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_\${gene}.rda,${output_rda}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_\${gene}.varianceRatio.txt >> for_conditioning.csv
            fi            
        done
    """
}


process SAIGE_S3 {
    label 'process_tiny'

    // Specify the number of forks (10k)
    // maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.saige_container}"
    } else {
        container "${params.saige_docker}"
    }    

    input:
        tuple val(name),path(genes_list),path(output)
    output:
        tuple val(name),path("output_2/*_cis_genePval"), emit: q_out_2
    // Define the Bash script to run for each array job
    // \${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_gene_1_cis_gene_1
    script:

    parts = name.split('___')
    chr = parts[-1]

    """
        mkdir output_2
        cat "${genes_list}" | while IFS= read -r gene1 || [ -n "\$gene1" ]
        do
            chr1=\$(echo "\${gene1}" | cut -f2)
            gene=\$(echo "\${gene1}" | cut -f1)
            step2prefix=output_${name}___\${chr1}/\${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_cis
            step3_gene_pvalue_qtl.R \
            --assocFile=\${step2prefix}_\${gene}        \
            --geneName=\${gene}       \
            --genePval_outputFile=output_2/nindep_100_ncell_100_lambda_chr\${chr1}_tauIntraSample_0.5_\${gene}_cis_genePval
        done
    """
}


process AGGREGATE_ACAT_RESULTS{
    tag { condition }
    scratch false      // use tmp directory
    label 'process_tiny'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    
    

    publishDir  path: "${params.outdir}/Saige_eQTLS/${exp}",
                mode: "${params.copy_mode}",
                overwrite: "true"

    input:
        tuple val(group),path(cis_genePval)
    output:
        tuple val(group),path("ACAT_all.tsv"), emit: acat_all
    script:
        parts = group.split('__')
        chr = parts[-1]
        exp = parts[-2] 
        """
            head -n 1 \$(find . -name "*_cis_genePval" | head -n 1)  >> ACAT_all.tsv
            find . -name "*_cis_genePval" -print0 | xargs -0 -I {} sh -c 'tail -n +2 "\$1"' sh {} >> ACAT_all.tsv
        """
}


process AGGREGATE_QTL_RESULTS{
    tag { condition }
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    
    

    publishDir  path: "${params.outdir}/Saige_eQTLS/${exp}",
                mode: "${params.copy_mode}",
                overwrite: "true"


    input:
        tuple val(group),path(qtl_q_results)
    output:
        tuple val(group),path("minimum_q_all_genes.tsv"), emit: all

    script:
        parts = group.split('__')
        chr = parts[-1]
        exp = parts[-2] 
        """
            prepend_gene.py --pattern 'cis_*_minimum_q.txt' --column 'gene' --outfile 'minimum_q_all_genes.tsv'
            qvalue_correction.py -f minimum_q_all_genes.tsv -c "18" -n "qvalues_across_genes" -w "FALSE"
        """
}

process AGGREGATE_QTL_ALLVARS{
    tag { condition }
    scratch false      // use tmp directory
    label 'process_low'
    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    
    

    publishDir  path: "${params.outdir}/Saige_eQTLS/${exp}",
                mode: "${params.copy_mode}",
                overwrite: "true"

    input:
        tuple val(group),path(qtl_q_results)
    output:
        tuple val(group),path("${chr}__${exp}__all_vars_genes.tsv.gz"), emit: all  optional true
        tuple val(group),path("p_vals_lambd*"), emit: lambda optional true
        tuple val(group),path("*gene_lambdas.tsv"), emit: lambda_vals optional true
    script:
        parts = group.split('__')
        chr = parts[-1]
        exp = parts[-3]

        """
            prepend_gene_large.py --pattern 'cis_*' --column 'gene' --outfile '${chr}__${exp}__all_vars_genes.tsv'
            gzip ${chr}__${exp}__all_vars_genes.tsv
            mv cell_lambdas.tsv ${chr}__${exp}__gene_lambdas.tsv || echo 'not existant'
        """
}

process PHENOTYPE_PCs{
    // label 'process_medium'
    tag { sanitized_columns }

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }   

    memory { 
        sizeInGB = saige_filt_expr_input.size() / 1e9 * 2 * task.attempt
        return (sizeInGB ).toString() + 'GB' 
    }   

    input:
        tuple val(sanitized_columns), path(saige_filt_expr_input),path(covariates)
        val(phenotype_pcs)

    output:
        tuple val(sanitized_columns), path("${sanitized_columns}_with_pheno_pcs.tsv"),path("covariates_new.txt"), emit: output_pheno optional true

    script:

    """
        export LD_LIBRARY_PATH=/opt/libstdc++-old:/usr/lib:/usr/local/lib:\$LD_LIBRARY_PATH
        export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:\$LD_LIBRARY_PATH
        saige_phenotype_pcs_and_other_covs.py ${saige_filt_expr_input} ${sanitized_columns}_with_pheno_pcs.tsv ${phenotype_pcs} ${covariates}
    """
}

process H5AD_TO_SAIGE_FORMAT {
    label 'process_medium'
    tag { sanitized_columns }

    // Specify the number of forks (10k)
    // maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }   

    memory { 
        sizeInGB = adata.size() / 1e9 * 1.25 * task.attempt
        return (sizeInGB ).toString() + 'GB' 
    }   
    publishDir  path: "${params.outdir}/Saige_eQTLS",
        saveAs: { filename ->
            if (filename.contains("covariates.txt")) {
                return null
            } else if (filename.contains("saige_filt_expr_input.tsv")) {
                return null
            } else if (filename.contains("test_genes.txt")) {
                return null
            } else if (filename.contains("output_agg/")) {
                // Assuming `filename` contains the full path including directories
                // Remove 'output_agg/azimuth.celltyp.l0/' from the path
                def newFilename = filename.replaceAll(".*output_agg/[^/]+/", "")
                return newFilename
            } else {
                return null
            }
        },
        mode: "${params.copy_mode}",
        overwrite: "true"
    input:
        each path(h5ad)  
        path(bridge)  
        val(aggregation_columns)
        path(genotype_pcs)
        path(genome_annotation)

    output:
        tuple val(sanitized_columns), path("output_agg/*/*/saige_filt_expr_input.tsv"),path("output_agg/*/*/covariates.txt"),emit:output_pheno optional true
        tuple val(sanitized_columns),path("output_agg/*/*/test_genes.txt"),emit:gene_chunk optional true
        path("output_agg/*"),emit:output_agg optional true


    // Define the Bash script to run for each array job
    script:
    sanitized_columns = h5ad.getName().replaceAll(/[^a-zA-Z0-9]/, '_').replaceAll(/\.h5ad$/, '')
    if("${params.SAIGE.covariate_obs_columns}"==''){
        cov_col = ""
    }else{
        cov_col = "--covariates ${params.SAIGE.covariate_obs_columns}"
    }
    sizeInGB = h5ad.size() / 1e9 * 3 + 5 * task.attempt

    if ("${params.aggregation_subentry}"==''){
        cond1 = " --condition_col 'NULL' --condition 'NULL' "
    }else{
        cond1 = " --condition_col '${aggregation_columns}' --condition '${params.aggregation_subentry}' "
    }

    if ("${params.chromosomes_to_test}"!=''){
        chromosomes_as_string = params.chromosomes_to_test.join(',')
        cond2 = " --chr ${chromosomes_as_string} --genome ${genome_annotation}"
    }else{
        cond2 = " "
    }

    """
        echo ${sizeInGB}
        bridge='${bridge}'
        nperc=${params.percent_of_population_expressed}
        condition_col="${aggregation_columns}" #Specify 'NULL' if want to include all cells
        condition="${aggregation_columns}" #Specify 'NULL' if want to include all cells
        
        scale_covariates=true
        expression_pca=${params.SAIGE.nr_expression_pcs}
        aggregate_on="${aggregation_columns}"

        mkdir output_agg
        prep_adata_saige.py \
            --phenotype__file ${h5ad} \
            --bridge \$bridge \
            --aggregate_on \$aggregate_on \
            --genotype_pc__file ${genotype_pcs} \
            --genotype_id ${params.gt_id_column} \
            --sample_id ${params.sample_column} \
            --general_file_dir ./output_agg \
            --nperc \$nperc \
            --gtf_gene_identifier ${params.gtf_gene_identifier} \
            --min ${params.n_min_cells} \
            --scale_covariates \$scale_covariates \
            --expression_pca \$expression_pca --cell_percentage_threshold ${params.cell_percentage_threshold} \
            ${cov_col} ${cond1} ${cond2}
    """
}


process TEST {
    label 'process_low'

    // Specify the number of forks (10k)
    // maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.saige_container}"
    } else {
        container "${params.saige_docker}"
    }    

    input:
        tuple val(sanitized_columns), path(saige_filt_expr_input),path(test_genes) 

    script:
    """
        echo ${sanitized_columns}
        echo ${test_genes}
    """
}


process CHUNK_GENES {
    label 'process_low'
    tag { sanitized_columns }
    // Specify the number of forks (10k)
    // maxForks 1000

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.saige_container}"
    } else {
        container "${params.saige_docker}"
    }    

    input:
        tuple val(sanitized_columns),path(test_genes) 
        val(chunk_size)
    output:
        tuple val(sanitized_columns), path("chunk_${sanitized_columns}_*"),emit:output_genes
        
    // Define the Bash script to run for each array job
    // \${chr1}___nindep_100_ncell_100_lambda_2_tauIntraSample_0.5_gene_1_cis_gene_1
    script:
    """
        echo ${sanitized_columns}
        echo ${test_genes}
        split -l ${chunk_size} ${test_genes} chunk_${sanitized_columns}_
    """
}

process DETERMINE_TSS_AND_TEST_REGIONS {
    label 'process_low'

    // Specify the number of forks (10k)
    maxForks 200

    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {
        container "${params.eqtl_container}"
    } else {
        container "${params.eqtl_docker}"
    }    

    input:
        tuple val(name),path(genes_list),path(output)
        each path(annotation_file)

    output:
        tuple val(name),path(genes_list),path(output),path('gene_regions_to_test.tsv'),emit:output_genes
        
    // We want to test in a ceirtain window of the gene TSS +/-, to do this we need to propeare SAIGE regions files. 
    script:
    """
        echo 'Lets prepeare the regions file'
        prepeare_Saige_regions_for_cis.py --annotation_file ${annotation_file} --genes ${genes_list} --gtf_type ${params.gtf_type} --window ${params.windowSize} --gtf_gene_identifier ${params.gtf_gene_identifier}
    """
}

workflow SAIGE_qtls{
    take:
        genotype_pcs
        phenotype_file
        bim_bed_fam
        genome_annotation
        genotype_phenotype_mapping_file

    main:
        log.info('------- Running SAIGE QTLs ------- ')
        pheno =  Channel.of()
        gene =  Channel.of()
        // Check if aggregation_subentry is provided
        if (params.SAIGE.aggregation_subentry != '') {
            log.info("------- Analysing ${params.SAIGE.aggregation_subentry} celltypes ------- ")
            // Split the aggregation_subentry parameter into a list of patterns
            valid_files = phenotype_file.filter { file ->
                params.SAIGE.aggregation_subentry.split(',').any { pattern -> "${file}".contains("__${pattern}__") }
                
            }
        } else {
            log.info('------- Analysing all celltypes ------- ')
            valid_files = phenotype_file
        }


        H5AD_TO_SAIGE_FORMAT(
            valid_files,
            genotype_phenotype_mapping_file,
            params.aggregation_columns,
            genotype_pcs,
            genome_annotation
        )
        pheno = H5AD_TO_SAIGE_FORMAT.out.output_pheno
        gene = H5AD_TO_SAIGE_FORMAT.out.gene_chunk

        PHENOTYPE_PCs(pheno,params.SAIGE.nr_expression_pcs)
        pheno = PHENOTYPE_PCs.out.output_pheno

        CHUNK_GENES(gene,params.chunkSize)
        result = CHUNK_GENES.out.output_genes.flatMap { item ->
            def (first, second) = item
            if (!(second instanceof Collection)) {
                second = [second] // Wrap single value in a list
            }
            return second.collect { [first, it] }
        }

        result.combine(pheno, by: 0).set{pheno_chunk}

        
        Channel.fromList(params.chromosomes_to_test)
                .set{chromosomes_to_test}        
        CREATE_SPARSE_GRM(bim_bed_fam)
        sparseGRM = CREATE_SPARSE_GRM.out.sparseGRM
        sparseGRM_sample = CREATE_SPARSE_GRM.out.sparseGRM_sample
        SAIGE_S1(pheno_chunk.combine(bim_bed_fam),sparseGRM,sparseGRM_sample)

        DETERMINE_TSS_AND_TEST_REGIONS(SAIGE_S1.out.output,genome_annotation)
        for_cis_input = DETERMINE_TSS_AND_TEST_REGIONS.out.output_genes
        SAIGE_S2_CIS(for_cis_input.combine(bim_bed_fam),sparseGRM,sparseGRM_sample)
        output_s2 = SAIGE_S2_CIS.out.output
        agg_output = SAIGE_S2_CIS.out.for_aggregation

        // HERE WE either run the cis or trans qtl mapping. For cis we loop through each of the chunks whereas in trans we can run all together.
        SAIGE_QVAL_COR(output_s2)
        SAIGE_S3(SAIGE_QVAL_COR.out.output)

        // ########## Collecting Chunk outputs.  ###############
        SAIGE_S2_for_aggregation = agg_output.flatMap { item ->
            def (first, second) = item
            if (!(second instanceof Collection)) {
                second = [second] // Wrap single value in a list
            }
            return second.collect { 
                def fileName = "${it}".split('/').last() // Extract the filename by splitting on '/'
                def number = fileName.split('__')[0] // Extract the number before the first '__'
                ["${first}__${number}", it] 
            }
        }

        SAIGE_S3_for_aggregation = SAIGE_QVAL_COR.out.q_out.flatMap { item ->
            def (first, second) = item
            if (!(second instanceof Collection)) {
                second = [second] // Wrap single value in a list
            }
            return second.collect { [first, it] }   
        }

        SAIGE_S3_for_aggregation_ACAT = SAIGE_S3.out.q_out_2.flatMap { item ->
            def (first, second) = item
            if (!(second instanceof Collection)) {
                second = [second] // Wrap single value in a list
            }
            return second.collect { [first, it] }
        }

        SAIGE_S3_for_aggregation_ACAT = SAIGE_S3_for_aggregation_ACAT.map{row->tuple("${row[0]}".replaceFirst(/___.*/,""),
                                                    file(row[1])
                                                    )}  

        SAIGE_S3_for_aggregation = SAIGE_S3_for_aggregation.map{row->tuple("${row[0]}".replaceFirst(/___.*/,""),
                                                    file(row[1])
                                                    )}  
        AGGREGATE_QTL_RESULTS(SAIGE_S3_for_aggregation.groupTuple(by: 0))
        AGGREGATE_QTL_ALLVARS(SAIGE_S2_for_aggregation.groupTuple(by: 0))
        AGGREGATE_ACAT_RESULTS(SAIGE_S3_for_aggregation_ACAT.groupTuple(by: 0))
        // CONDITIONAL_QTL(SAIGE_QVAL_COR.out.for_conditioning)

}-e 
===== /lustre/scratch124/humgen/projects_v2/cardinal_analysis/analysis/mo11/mo11_tmp_work/qtlight_tests/v7__add_jax/QTLight_v1.80/modules/local/saige/functions.nf =====
